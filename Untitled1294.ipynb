{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e3a895-7383-4003-8c51-e22b94d8abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building DeepFakeShield image manifest...\n",
      "[INFO] train/real: processed 200 images...\n",
      "[INFO] train/real: total 326 images\n",
      "[INFO] train/fake: total 153 images\n",
      "[INFO] sample/fake: total 5 images\n",
      "[INFO] test/fake: processed 200 images...\n",
      "[INFO] test/fake: total 389 images\n",
      "[INFO] test/real: total 110 images\n",
      "[INFO] Manifest rows: 983\n",
      "[INFO] Columns: ['split', 'label_name', 'label', 'filename', 'relpath', 'abspath', 'ext', 'size_bytes', 'sha256_head', 'width', 'height', 'mode', 'format', 'dpi_x', 'dpi_y', 'source_dir']\n",
      "[INFO] Head:\n",
      "     split label_name  label                 filename                  relpath  \\\n",
      "0  sample       fake      1  IMG-20250106-WA0009.jpg  IMG-20250106-WA0009.jpg   \n",
      "1  sample       fake      1  IMG-20250106-WA0010.jpg  IMG-20250106-WA0010.jpg   \n",
      "2  sample       fake      1  IMG-20250106-WA0011.jpg  IMG-20250106-WA0011.jpg   \n",
      "3  sample       fake      1  IMG-20250106-WA0012.jpg  IMG-20250106-WA0012.jpg   \n",
      "4  sample       fake      1  IMG-20250106-WA0013.jpg  IMG-20250106-WA0013.jpg   \n",
      "\n",
      "                                             abspath   ext  size_bytes  \\\n",
      "0  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  .jpg       74687   \n",
      "1  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  .jpg       48504   \n",
      "2  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  .jpg       64257   \n",
      "3  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  .jpg       68513   \n",
      "4  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  .jpg       67391   \n",
      "\n",
      "                                         sha256_head  width  height mode  \\\n",
      "0  a9f36970bdbeda97d03a890d1d7b195041e9b0b7796ff6...    960     720  RGB   \n",
      "1  834f446d38c2ac5ae09ce0446f0a81ba1d6c9682f1f021...    960     720  RGB   \n",
      "2  ae69cd5bd4a9e5e328187c6db36e0ecfb48e299a4ef86f...    960     720  RGB   \n",
      "3  61ebe42c01eb7b7d2d9e36960fba339aad4d5b2586863a...    960     720  RGB   \n",
      "4  2545227bee02623c13ad21493686b6c39e7ddaa6597637...    960     720  RGB   \n",
      "\n",
      "  format dpi_x dpi_y                                         source_dir  \n",
      "0   JPEG  None  None  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  \n",
      "1   JPEG  None  None  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  \n",
      "2   JPEG  None  None  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  \n",
      "3   JPEG  None  None  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  \n",
      "4   JPEG  None  None  C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archiv...  \n",
      "[WRITE] Pickle  -> C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.pkl\n",
      "[WRITE] HDF5    -> C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.h5\n",
      "[WRITE] JSON    -> C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.json\n",
      "[WRITE] YAML    -> C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.yaml\n",
      "\n",
      "[DONE] Files saved in: C:\\Users\\sagni\\Downloads\\DeepFakeShield\n",
      " - C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.pkl\n",
      " - C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.h5\n",
      " - C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.json\n",
      " - C:\\Users\\sagni\\Downloads\\DeepFakeShield\\deepfakeshield_images.yaml\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# DeepFakeShield — Image Manifest -> PKL / H5 / JSON / YAML\n",
    "# ==============================================================\n",
    "# Scans these folders (recursive), labels them, and saves a manifest:\n",
    "#   Train/Real:\n",
    "#     C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\train-20250112T065955Z-001\\train\\real\n",
    "#   Train/Fake:\n",
    "#     C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\train-20250112T065955Z-001\\train\\fake\n",
    "#   Sample/Fake:\n",
    "#     C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\Sample_fake_images\\Sample_fake_images\\fake\n",
    "#   Test/Fake:\n",
    "#     C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\test-20250112T065939Z-001\\test\\fake\n",
    "#   Test/Real:\n",
    "#     C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\test-20250112T065939Z-001\\test\\real\n",
    "#\n",
    "# Outputs (saved to C:\\Users\\sagni\\Downloads\\DeepFakeShield):\n",
    "#   - deepfakeshield_images.pkl\n",
    "#   - deepfakeshield_images.h5\n",
    "#   - deepfakeshield_images.json\n",
    "#   - deepfakeshield_images.yaml\n",
    "#\n",
    "# Notes:\n",
    "# - We store paths + metadata (no image bytes).\n",
    "# - HDF5 requires `tables` (PyTables, 64-bit Python recommended).\n",
    "# - JSON/YAML have NaNs converted to None, and pure Python scalars.\n",
    "# ==============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Optional YAML ----------\n",
    "try:\n",
    "    import yaml\n",
    "    HAVE_YAML = True\n",
    "except Exception:\n",
    "    HAVE_YAML = False\n",
    "    print(\"[WARN] PyYAML not installed; YAML output will be skipped.\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "OUT_DIR = Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folders to scan (path, split, label_name) — label: real=0, fake=1\n",
    "DATA_DIRS: List[Tuple[Path, str, str]] = [\n",
    "    (Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\train-20250112T065955Z-001\\train\\real\"), \"train\", \"real\"),\n",
    "    (Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\train-20250112T065955Z-001\\train\\fake\"), \"train\", \"fake\"),\n",
    "    (Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\Sample_fake_images\\Sample_fake_images\\fake\"), \"sample\", \"fake\"),\n",
    "    (Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\test-20250112T065939Z-001\\test\\fake\"), \"test\", \"fake\"),\n",
    "    (Path(r\"C:\\Users\\sagni\\Downloads\\DeepFakeShield\\archive\\test-20250112T065939Z-001\\test\\real\"), \"test\", \"real\"),\n",
    "]\n",
    "\n",
    "BASE = \"deepfakeshield_images\"\n",
    "PKL_PATH  = OUT_DIR / f\"{BASE}.pkl\"\n",
    "H5_PATH   = OUT_DIR / f\"{BASE}.h5\"\n",
    "JSON_PATH = OUT_DIR / f\"{BASE}.json\"\n",
    "YAML_PATH = OUT_DIR / f\"{BASE}.yaml\"\n",
    "\n",
    "# ---------- Config ----------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".webp\", \".gif\"}\n",
    "HASH_BYTES = 1024 * 1024  # 1 MB fingerprint for quick dedupe\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def fast_sha256(path: Path, n_bytes: int = HASH_BYTES) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            h.update(f.read(n_bytes))\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_image_meta(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Open image and extract safe metadata without loading full image.\"\"\"\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            im.load()  # ensure header is read\n",
    "            w, h = im.size\n",
    "            dpi_x, dpi_y = None, None\n",
    "            dpi = im.info.get(\"dpi\")\n",
    "            if isinstance(dpi, (tuple, list)) and len(dpi) >= 2:\n",
    "                dpi_x, dpi_y = dpi[0], dpi[1]\n",
    "            return {\n",
    "                \"width\": w,\n",
    "                \"height\": h,\n",
    "                \"mode\": im.mode,\n",
    "                \"format\": im.format,\n",
    "                \"dpi_x\": dpi_x,\n",
    "                \"dpi_y\": dpi_y,\n",
    "            }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"width\": None, \"height\": None, \"mode\": None, \"format\": None,\n",
    "            \"dpi_x\": None, \"dpi_y\": None\n",
    "        }\n",
    "\n",
    "def list_images(root: Path) -> List[Path]:\n",
    "    if not root.exists():\n",
    "        print(f\"[WARN] Missing directory (skipped): {root}\")\n",
    "        return []\n",
    "    return [p for p in root.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "\n",
    "def scan_dir(root: Path, split: str, label_name: str) -> List[Dict[str, Any]]:\n",
    "    label = 1 if label_name.lower() == \"fake\" else 0\n",
    "    files = list_images(root)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for i, p in enumerate(files, 1):\n",
    "        try:\n",
    "            st = p.stat()\n",
    "            meta = safe_image_meta(p)\n",
    "            rows.append({\n",
    "                \"split\": split,                      # train / test / sample\n",
    "                \"label_name\": label_name,            # \"real\" / \"fake\"\n",
    "                \"label\": int(label),                 # 0 / 1\n",
    "                \"filename\": p.name,\n",
    "                \"relpath\": str(p.relative_to(root)),\n",
    "                \"abspath\": str(p.resolve()),\n",
    "                \"ext\": p.suffix.lower(),\n",
    "                \"size_bytes\": int(st.st_size),\n",
    "                \"sha256_head\": fast_sha256(p),\n",
    "                \"width\": meta[\"width\"],\n",
    "                \"height\": meta[\"height\"],\n",
    "                \"mode\": meta[\"mode\"],\n",
    "                \"format\": meta[\"format\"],\n",
    "                \"dpi_x\": meta[\"dpi_x\"],\n",
    "                \"dpi_y\": meta[\"dpi_y\"],\n",
    "                \"source_dir\": str(root),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"split\": split,\n",
    "                \"label_name\": label_name,\n",
    "                \"label\": int(label),\n",
    "                \"filename\": p.name,\n",
    "                \"relpath\": str(p.relative_to(root)),\n",
    "                \"abspath\": str(p.resolve()),\n",
    "                \"ext\": p.suffix.lower(),\n",
    "                \"size_bytes\": None,\n",
    "                \"sha256_head\": \"\",\n",
    "                \"width\": None, \"height\": None, \"mode\": None, \"format\": None,\n",
    "                \"dpi_x\": None, \"dpi_y\": None,\n",
    "                \"source_dir\": str(root),\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "        if i % 200 == 0:\n",
    "            print(f\"[INFO] {split}/{label_name}: processed {i} images...\")\n",
    "    print(f\"[INFO] {split}/{label_name}: total {len(files)} images\")\n",
    "    return rows\n",
    "\n",
    "def build_manifest(dirs: List[Tuple[Path, str, str]]) -> pd.DataFrame:\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    for root, split, label_name in dirs:\n",
    "        all_rows.extend(scan_dir(root, split, label_name))\n",
    "    if not all_rows:\n",
    "        print(\"[WARN] No images found in any provided directory.\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"split\", \"label_name\", \"label\", \"filename\", \"relpath\", \"abspath\", \"ext\",\n",
    "            \"size_bytes\", \"sha256_head\", \"width\", \"height\", \"mode\", \"format\",\n",
    "            \"dpi_x\", \"dpi_y\", \"source_dir\", \"error\"\n",
    "        ])\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    # Stable ordering for repeatable outputs\n",
    "    order_cols = [\n",
    "        \"split\", \"label_name\", \"label\", \"filename\", \"relpath\", \"abspath\", \"ext\",\n",
    "        \"size_bytes\", \"sha256_head\", \"width\", \"height\", \"mode\", \"format\",\n",
    "        \"dpi_x\", \"dpi_y\", \"source_dir\", \"error\"\n",
    "    ]\n",
    "    df = df[[c for c in order_cols if c in df.columns]].sort_values(\n",
    "        [\"split\", \"label_name\", \"source_dir\", \"relpath\", \"filename\"]\n",
    "    ).reset_index(drop=True)\n",
    "    print(f\"[INFO] Manifest rows: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# ---------- Coercion for storage ----------\n",
    "NUMERIC_COLS = [\"label\", \"size_bytes\", \"width\", \"height\", \"dpi_x\", \"dpi_y\"]\n",
    "\n",
    "def coerce_for_hdf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure PyTables-friendly dtypes: numeric -> float64/int64, strings for others.\"\"\"\n",
    "    g = df.copy()\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in g.columns:\n",
    "            g[c] = pd.to_numeric(g[c], errors=\"coerce\").astype(\"float64\")\n",
    "    # Ensure object/string for non-numeric\n",
    "    for c in g.columns:\n",
    "        if c not in NUMERIC_COLS:\n",
    "            g[c] = g[c].where(g[c].notna(), None).astype(object)\n",
    "    return g\n",
    "\n",
    "def records_for_json_yaml(df: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"Return list of Python-native dicts (NaN->None) suitable for json/yaml.\"\"\"\n",
    "    g = df.copy()\n",
    "    # Convert numeric columns to float64 for uniformity\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in g.columns:\n",
    "            g[c] = pd.to_numeric(g[c], errors=\"coerce\").astype(\"float64\")\n",
    "    # Replace NaN/NA with None\n",
    "    g = g.where(pd.notna(g), None)\n",
    "    recs = g.to_dict(orient=\"records\")\n",
    "\n",
    "    # Convert numpy scalars to Python types\n",
    "    out = []\n",
    "    for rec in recs:\n",
    "        clean = {}\n",
    "        for k, v in rec.items():\n",
    "            if isinstance(v, (np.floating,)):\n",
    "                clean[k] = float(v)\n",
    "            elif isinstance(v, (np.integer,)):\n",
    "                clean[k] = int(v)\n",
    "            elif isinstance(v, (np.bool_,)):\n",
    "                clean[k] = bool(v)\n",
    "            else:\n",
    "                clean[k] = v  # str, None, etc.\n",
    "        out.append(clean)\n",
    "    return out\n",
    "\n",
    "def save_all_formats(df: pd.DataFrame):\n",
    "    # 1) PKL\n",
    "    print(f\"[WRITE] Pickle  -> {PKL_PATH}\")\n",
    "    df.to_pickle(PKL_PATH)\n",
    "\n",
    "    # 2) HDF5\n",
    "    try:\n",
    "        df_h5 = coerce_for_hdf(df)\n",
    "        print(f\"[WRITE] HDF5    -> {H5_PATH}\")\n",
    "        df_h5.to_hdf(H5_PATH, key=\"images\", mode=\"w\", format=\"table\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not write HDF5: {e}\\n       Hint: ensure `pip install tables` and 64-bit Python.\")\n",
    "\n",
    "    # 3) JSON\n",
    "    print(f\"[WRITE] JSON    -> {JSON_PATH}\")\n",
    "    records = records_for_json_yaml(df)\n",
    "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) YAML\n",
    "    if HAVE_YAML:\n",
    "        try:\n",
    "            print(f\"[WRITE] YAML    -> {YAML_PATH}\")\n",
    "            with open(YAML_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                yaml.safe_dump(records, f, allow_unicode=True, sort_keys=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not write YAML: {e}\")\n",
    "    else:\n",
    "        print(\"[INFO] Skipping YAML (PyYAML not installed).\")\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] Building DeepFakeShield image manifest...\")\n",
    "    manifest_df = build_manifest(DATA_DIRS)\n",
    "    print(\"[INFO] Columns:\", list(manifest_df.columns))\n",
    "    print(\"[INFO] Head:\\n\", manifest_df.head(5))\n",
    "    save_all_formats(manifest_df)\n",
    "\n",
    "    print(\"\\n[DONE] Files saved in:\", OUT_DIR)\n",
    "    print(\" -\", PKL_PATH)\n",
    "    print(\" -\", H5_PATH)\n",
    "    print(\" -\", JSON_PATH)\n",
    "    if HAVE_YAML:\n",
    "        print(\" -\", YAML_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b11290-79c2-40c7-a5ec-2938a6a954b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
